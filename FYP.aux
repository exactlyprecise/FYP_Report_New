\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*{\memsetcounter}[2]{}
\bibstyle{sp}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{american}{}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{Acknowledgement}{v}{chapter*.1}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{Abstract}{vii}{chapter*.2}}
\@writefile{toc}{\contentsline {chapter}{Contents}{ix}{section*.3}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {1}Introduction}{1}{chapter.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Supervised Learning}{1}{section.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Bias Variance Tradeoff}{1}{section.1.2}}
\citation{Belkin_2019}
\citation{Zhang_Deep_Learning}
\citation{krizhevsky2009learning}
\citation{Russakovsky_2015}
\citation{Alfredo2016}
\newlabel{eqn:Generalization_Gap}{{1.1}{2}{Bias Variance Tradeoff}{equation.1.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Modern Machine Learning}{2}{section.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Research Question}{2}{section.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Curve showing how the training and test risk changes with respect to the capacity of $\mathcal  {H}$. The test risk results from bias-variance tradeoff, and the Capacity of $\mathcal  {H}$ is selected at the sweet spot.}}{3}{figure.1.1}}
\newlabel{fig:Classical_Descent}{{\M@TitleReference {1.1}{Curve showing how the training and test risk changes with respect to the capacity of $\mathcal  {H}$. The test risk results from bias-variance tradeoff, and the Capacity of $\mathcal  {H}$ is selected at the sweet spot.}}{3}{Curve showing how the training and test risk changes with respect to the capacity of $\HH $. The test risk results from bias-variance tradeoff, and the Capacity of $\HH $ is selected at the sweet spot}{figure.1.1}{}}
\citation{steinwartSVM}
\citation{steinwartSVM}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {2}Kernels}{5}{chapter.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Notation}{5}{section.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Definition and Properties}{5}{section.2.2}}
\newlabel{lem:kernelSymm}{{\M@TitleReference {1}{Definition and Properties}}{5}{}{lem.1}{}}
\citation{steinwartSVM}
\newlabel{eqn:innerKernel}{{2.2}{7}{Definition and Properties}{equation.2.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Reproducing Kernel Hilbert Spaces}{7}{section.2.3}}
\newlabel{sec:RKHS}{{\M@TitleReference {2.3}{Reproducing Kernel Hilbert Spaces}}{7}{Reproducing Kernel Hilbert Spaces}{section.2.3}{}}
\newlabel{eqn:ReproducingProp}{{2.3}{8}{}{equation.2.3.3}{}}
\newlabel{def:CanFeatureMaps}{{\M@TitleReference {7}{Reproducing Kernel Hilbert Spaces}}{8}{}{defn.7}{}}
\citation{Representer_Theorem}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Representer Theorem}{9}{subsection.2.3.1}}
\newlabel{subsec:RepThm}{{\M@TitleReference {2.3.1}{Representer Theorem}}{9}{Representer Theorem}{subsection.2.3.1}{}}
\newlabel{thm:Representer}{{\M@TitleReference {2}{Representer Theorem}}{9}{}{thm.2.3.2}{}}
\citation{UnderstandKernel}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {3}Overfitted and Interpolated Kernel Classifiers}{13}{chapter.3}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {4}Existing Bounds}{15}{chapter.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Existing Bounds Provide No Guarantees for Interpolated Kernel Classifiers}{15}{section.4.1}}
\newlabel{sec:BoundsKernel}{{\M@TitleReference {4.1}{Existing Bounds Provide No Guarantees for Interpolated Kernel Classifiers}}{15}{Existing Bounds Provide No Guarantees for Interpolated Kernel Classifiers}{section.4.1}{}}
\newlabel{thm:normBound}{{\M@TitleReference {3}{Existing Bounds Provide No Guarantees for Interpolated Kernel Classifiers}}{15}{}{thm.4.1.3}{}}
\citation{LossFATBound}
\citation{ApproximationConcentration}
\citation{UnderstandKernel}
\citation{Belkin_2019}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {5}Double Descent}{17}{chapter.5}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Double Descent Curve}{17}{section.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Curve showing the double descent proposition. When the capacity of $\mathcal  {H}$ is large enough the test risk may be lower than that at the local minimum.}}{17}{figure.5.1}}
\newlabel{fig:Double_Descent}{{\M@TitleReference {5.1}{Curve showing the double descent proposition. When the capacity of $\mathcal  {H}$ is large enough the test risk may be lower than that at the local minimum.}}{17}{Curve showing the double descent proposition. When the capacity of $\HH $ is large enough the test risk may be lower than that at the local minimum}{figure.5.1}{}}
\citation{RFF_Rahimi}
\citation{Rudin_1990}
\citation{RFF_Rahimi}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Random Fourier Features}{18}{section.5.2}}
\newlabel{sec:RFFs}{{\M@TitleReference {5.2}{Random Fourier Features}}{18}{Random Fourier Features}{section.5.2}{}}
\newlabel{eq:probFourier}{{5.1}{18}{Random Fourier Features}{equation.5.2.1}{}}
\newlabel{thm:Bochner}{{\M@TitleReference {4}{Random Fourier Features}}{18}{}{thm.5.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Experiment}{20}{section.5.3}}
\newlabel{sec:RFF_Exp}{{\M@TitleReference {5.3}{Experiment}}{20}{Experiment}{section.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Function Class}{20}{subsection.5.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Algorithm}{20}{subsection.5.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Results}{21}{subsection.5.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Plausible Explanations}{21}{section.5.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Experimental Results: RFF function class on MNIST. $n=10^4$, test risk and coefficient norm is in logarithmic scale.}}{22}{figure.5.2}}
\newlabel{fig:RFF_Exp}{{\M@TitleReference {5.2}{Experimental Results: RFF function class on MNIST. $n=10^4$, test risk and coefficient norm is in logarithmic scale.}}{22}{Experimental Results: RFF function class on MNIST. $n=10^4$, test risk and coefficient norm is in logarithmic scale}{figure.5.2}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {6}Approximation and Estimation}{23}{chapter.6}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Notations}{23}{section.6.1}}
\newlabel{sec:Notations}{{\M@TitleReference {6.1}{Notations}}{23}{Notations}{section.6.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Interpolation Estimates}{23}{section.6.2}}
\newlabel{eqn:InterpolateMat}{{6.1}{23}{Interpolation Estimates}{equation.6.2.1}{}}
\newlabel{eq:ustar}{{6.2}{24}{Interpolation Estimates}{equation.6.2.2}{}}
\newlabel{eqn:finVx}{{6.3}{24}{Interpolation Estimates}{equation.6.2.3}{}}
\citation{ScatteredDataApproximation}
\newlabel{eqn:vstar}{{6.5}{25}{}{equation.6.2.5}{}}
\citation{ScatteredDataApproximation}
\citation{ScatteredDataApproximation}
\newlabel{eqn:InterDecomp}{{6.6}{26}{Interpolation Estimates}{equation.6.2.6}{}}
\newlabel{thm:PowerBound}{{\M@TitleReference {6}{Interpolation Estimates}}{26}{}{thm.6.2.6}{}}
\citation{ScatteredDataApproximation}
\citation{ScatteredDataApproximation}
\newlabel{thm:interpolate}{{\M@TitleReference {7}{Interpolation Estimates}}{27}{}{thm.6.2.7}{}}
\citation{ScatteredDataApproximation}
\newlabel{eqn:P2bound}{{6.9}{28}{Interpolation Estimates}{equation.6.2.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Approximation Theorem}{30}{section.6.3}}
\newlabel{sec:AppThm}{{\M@TitleReference {6.3}{Approximation Theorem}}{30}{Approximation Theorem}{section.6.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Fill Distance of points in a Cube}{30}{subsection.6.3.1}}
\newlabel{thm:approx}{{\M@TitleReference {9}{Fill Distance of points in a Cube}}{31}{}{thm.6.3.9}{}}
\citation{ScatteredDataApproximation}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Regularity of Functions}{32}{subsection.6.3.2}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {7}Other Notes}{35}{chapter.7}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Weaknesses}{35}{section.7.1}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {A}Your first appendix}{37}{appendix.A}}
\bibdata{el-ht.bib}
\bibcite{ApproximationConcentration}{{1}{2018}{{Belkin}}{{}}}
\bibcite{Belkin_2019}{{2}{2019}{{Belkin et~al.}}{{Belkin, Hsu, Ma \& Mandal}}}
\bibcite{UnderstandKernel}{{3}{2018}{{Belkin et~al.}}{{Belkin, Ma \& Mandal}}}
\bibcite{Alfredo2016}{{4}{2016}{{Canziani et~al.}}{{Canziani, Paszke \& Culurciello}}}
\bibcite{LossFATBound}{{5}{2001}{{K{\'{e}}gl et~al.}}{{K{\'{e}}gl, Linder \& Lugosi}}}
\bibcite{krizhevsky2009learning}{{6}{2009}{{Krizhevsky \& Hinton}}{{}}}
\bibcite{RFF_Rahimi}{{7}{2008}{{Rahimi \& Recht}}{{}}}
\bibcite{Rudin_1990}{{8}{1990}{{Rudin}}{{}}}
\bibcite{Russakovsky_2015}{{9}{2015}{{Russakovsky et~al.}}{{Russakovsky, Deng, Su, Krause, Satheesh, Ma, Huang, Karpathy, Khosla, Bernstein, Berg \& Fei-Fei}}}
\bibcite{Representer_Theorem}{{10}{2001}{{Sch\IeC {\"o}lkopf et~al.}}{{Sch\IeC {\"o}lkopf, Herbrich \& Smola}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{39}{section*.5}}
\bibcite{steinwartSVM}{{11}{2008}{{Steinwart \& Christmann}}{{}}}
\bibcite{ScatteredDataApproximation}{{12}{2004}{{Wendland}}{{}}}
\bibcite{Zhang_Deep_Learning}{{13}{2016}{{Zhang et~al.}}{{Zhang, Bengio, Hardt, Recht \& Vinyals}}}
\bibstyle{sp}
\memsetcounter{lastsheet}{50}
\memsetcounter{lastpage}{40}
