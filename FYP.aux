\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*{\memsetcounter}[2]{}
\bibstyle{sp}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{american}{}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{Acknowledgement}{iii}{chapter*.1}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{Abstract}{v}{chapter*.2}}
\@writefile{toc}{\contentsline {chapter}{Contents}{vii}{section*.3}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {1}Introduction}{1}{chapter.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Supervised Learning}{1}{section.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Bias Variance Tradeoff}{1}{section.1.2}}
\citation{Belkin_2019}
\citation{Zhang_Deep_Learning}
\citation{krizhevsky2009learning}
\citation{Russakovsky_2015}
\citation{Alfredo2016}
\newlabel{eqn:Generalization_Gap}{{1.1}{2}{Bias Variance Tradeoff}{equation.1.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Modern Machine Learning}{2}{section.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Research Question}{2}{section.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Classical U-shaped curve showing how the training and test risk changes with respect to the capacity of $\mathcal  {H}$. The test risk results from bias-variance tradeoff, and the Capacity of $\mathcal  {H}$ is selected at the sweet spot.}}{3}{figure.1.1}}
\newlabel{fig:Classical_Descent}{{\M@TitleReference {1.1}{Classical U-shaped curve showing how the training and test risk changes with respect to the capacity of $\mathcal  {H}$. The test risk results from bias-variance tradeoff, and the Capacity of $\mathcal  {H}$ is selected at the sweet spot.}}{3}{Classical U-shaped curve showing how the training and test risk changes with respect to the capacity of $\HH $. The test risk results from bias-variance tradeoff, and the Capacity of $\HH $ is selected at the sweet spot}{figure.1.1}{}}
\citation{steinwartSVM}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {2}Kernels}{5}{chapter.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Notation}{5}{section.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Definition and Properties}{5}{section.2.2}}
\newlabel{lem:kernelSymm}{{\M@TitleReference {1}{Definition and Properties}}{5}{}{lem.1}{}}
\newlabel{defn:Gaussian_Kernel}{{\M@TitleReference {2}{Definition and Properties}}{5}{}{defn.2}{}}
\citation{steinwartSVM}
\citation{steinwartSVM}
\newlabel{eqn:innerKernel}{{2.2}{7}{Definition and Properties}{equation.2.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Reproducing Kernel Hilbert Spaces}{8}{section.2.3}}
\newlabel{sec:RKHS}{{\M@TitleReference {2.3}{Reproducing Kernel Hilbert Spaces}}{8}{Reproducing Kernel Hilbert Spaces}{section.2.3}{}}
\newlabel{eqn:ReproducingProp}{{2.3}{8}{}{equation.2.3.3}{}}
\newlabel{def:CanFeatureMaps}{{\M@TitleReference {8}{Reproducing Kernel Hilbert Spaces}}{8}{}{defn.8}{}}
\citation{Representer_Theorem}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Representer Theorem}{9}{subsection.2.3.1}}
\newlabel{subsec:RepThm}{{\M@TitleReference {2.3.1}{Representer Theorem}}{9}{Representer Theorem}{subsection.2.3.1}{}}
\newlabel{thm:Representer}{{\M@TitleReference {2}{Representer Theorem}}{9}{}{thm.2.3.2}{}}
\citation{UnderstandKernel}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {3}Overfitted and Interpolated Kernel Classifiers}{13}{chapter.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Rationale}{13}{section.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Training}{13}{section.3.2}}
\newlabel{eqn:interpolation_solution}{{3.1}{13}{Training}{equation.3.2.1}{}}
\citation{Zhang_Deep_Learning}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Results}{14}{section.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Existing Bounds for Interpolated Kernel Classifiers}{14}{section.3.4}}
\newlabel{sec:BoundsKernel}{{\M@TitleReference {3.4}{Existing Bounds for Interpolated Kernel Classifiers}}{14}{Existing Bounds for Interpolated Kernel Classifiers}{section.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces "ce" refers to classification error, and mse refers to the mean square loss. All methods resulted in 0\% classification error on the training set. All datasets were subsampled to reduce complexity required to train.}}{15}{figure.3.1}}
\newlabel{fig:Interpolated_Kernel}{{\M@TitleReference {3.1}{"ce" refers to classification error, and mse refers to the mean square loss. All methods resulted in 0\% classification error on the training set. All datasets were subsampled to reduce complexity required to train.}}{15}{"ce" refers to classification error, and mse refers to the mean square loss. All methods resulted in 0\% classification error on the training set. All datasets were subsampled to reduce complexity required to train}{figure.3.1}{}}
\citation{UnderstandKernel}
\citation{LossFATBound}
\newlabel{thm:normBound}{{\M@TitleReference {3}{Existing Bounds for Interpolated Kernel Classifiers}}{16}{}{thm.3.4.3}{}}
\citation{ApproximationConcentration}
\citation{UnderstandKernel,steinwartSVM,Rudi_2015}
\citation{Belkin_2019}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {4}Double Descent}{19}{chapter.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Double Descent Curve}{19}{section.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Curve showing the double descent proposition. When the capacity of $\mathcal  {H}$ is large enough the test risk may be lower than that at the local minimum.}}{19}{figure.4.1}}
\newlabel{fig:Double_Descent}{{\M@TitleReference {4.1}{Curve showing the double descent proposition. When the capacity of $\mathcal  {H}$ is large enough the test risk may be lower than that at the local minimum.}}{19}{Curve showing the double descent proposition. When the capacity of $\HH $ is large enough the test risk may be lower than that at the local minimum}{figure.4.1}{}}
\citation{RFF_Rahimi}
\citation{Rudin_1990}
\citation{RFF_Rahimi}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Random Fourier Features}{20}{section.4.2}}
\newlabel{sec:RFFs}{{\M@TitleReference {4.2}{Random Fourier Features}}{20}{Random Fourier Features}{section.4.2}{}}
\newlabel{eq:probFourier}{{4.1}{20}{Random Fourier Features}{equation.4.2.1}{}}
\newlabel{thm:Bochner}{{\M@TitleReference {4}{Random Fourier Features}}{20}{}{thm.4.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Experiment}{22}{section.4.3}}
\newlabel{sec:RFF_Exp}{{\M@TitleReference {4.3}{Experiment}}{22}{Experiment}{section.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Function Class}{22}{subsection.4.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Algorithm}{22}{subsection.4.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Results}{23}{subsection.4.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Plausible Explanations}{23}{section.4.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Experimental Results: RFF function class on MNIST. $n=10^4$, test risk and coefficient norm is in logarithmic scale.}}{24}{figure.4.2}}
\newlabel{fig:RFF_Exp}{{\M@TitleReference {4.2}{Experimental Results: RFF function class on MNIST. $n=10^4$, test risk and coefficient norm is in logarithmic scale.}}{24}{Experimental Results: RFF function class on MNIST. $n=10^4$, test risk and coefficient norm is in logarithmic scale}{figure.4.2}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {5}Approximation and Estimation}{25}{chapter.5}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Notations and Definitions}{25}{section.5.1}}
\newlabel{sec:Notations}{{\M@TitleReference {5.1}{Notations and Definitions}}{25}{Notations and Definitions}{section.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Interpolation Estimates}{25}{section.5.2}}
\newlabel{eqn:InterpolateMat}{{5.1}{25}{Interpolation Estimates}{equation.5.2.1}{}}
\newlabel{eq:ustar}{{5.2}{26}{Interpolation Estimates}{equation.5.2.2}{}}
\newlabel{eqn:finVx}{{5.3}{26}{Interpolation Estimates}{equation.5.2.3}{}}
\newlabel{eqn:vstar}{{5.5}{27}{}{equation.5.2.5}{}}
\citation{ScatteredDataApproximation}
\citation{ScatteredDataApproximation}
\newlabel{eqn:InterDecomp}{{5.6}{28}{Interpolation Estimates}{equation.5.2.6}{}}
\newlabel{thm:PowerBound}{{\M@TitleReference {6}{Interpolation Estimates}}{28}{}{thm.5.2.6}{}}
\citation{ScatteredDataApproximation}
\newlabel{thm:interpolate}{{\M@TitleReference {7}{Interpolation Estimates}}{29}{}{thm.5.2.7}{}}
\citation{ScatteredDataApproximation}
\newlabel{eqn:P2bound}{{5.9}{30}{Interpolation Estimates}{equation.5.2.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Approximation Theorem}{32}{section.5.3}}
\newlabel{sec:AppThm}{{\M@TitleReference {5.3}{Approximation Theorem}}{32}{Approximation Theorem}{section.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Fill Distance of points in a Cube}{32}{subsection.5.3.1}}
\newlabel{thm:approx}{{\M@TitleReference {9}{Fill Distance of points in a Cube}}{33}{}{thm.5.3.9}{}}
\citation{ScatteredDataApproximation}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Regularity of Functions}{34}{subsection.5.3.2}}
\citation{Shalev_Shwartz_2010}
\citation{Random_Forests_2017}
\citation{Belkin_Overfitting}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {6}Other Notes}{37}{chapter.6}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Weaknesses}{37}{section.6.1}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {A}Appendix}{39}{appendix.A}}
\newlabel{chpt:Appendix}{{\M@TitleReference {A}{Appendix}}{39}{Appendix}{appendix.A}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Results with Artificial Data and Noise}{39}{section.A.1}}
\newlabel{sec:AritificialData}{{\M@TitleReference {A.1}{Results with Artificial Data and Noise}}{39}{Results with Artificial Data and Noise}{section.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.1}Setup}{39}{subsection.A.1.1}}
\citation{Breiman_2001}
\citation{Cutler_2001}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces Left: Noiseless labels. Right: SNR=20. Top: Test MSE. Bottom: norm of function $\norm  {h}_\mathcal  {H}$.}}{40}{figure.A.1}}
\newlabel{fig:Synthetic_Exp}{{\M@TitleReference {A.1}{Left: Noiseless labels. Right: SNR=20. Top: Test MSE. Bottom: norm of function $\norm  {h}_\mathcal  {H}$.}}{40}{Left: Noiseless labels. Right: SNR=20. Top: Test MSE. Bottom: norm of function $\norm {h}_\HH $}{figure.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.2}Results}{40}{subsection.A.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Results with Random Forests}{40}{section.A.2}}
\newlabel{sec:RandomForests}{{\M@TitleReference {A.2}{Results with Random Forests}}{40}{Results with Random Forests}{section.A.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.1}Setup}{40}{subsection.A.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces Model capacity altered with $N_{leaf}^{max}$ and $N_{tree}$.(A): Bootstrap re-sampling disabled. (B): Bootstrap re-sampling enabled.}}{41}{figure.A.2}}
\newlabel{fig:Random_Forests}{{\M@TitleReference {A.2}{Model capacity altered with $N_{leaf}^{max}$ and $N_{tree}$.(A): Bootstrap re-sampling disabled. (B): Bootstrap re-sampling enabled.}}{41}{Model capacity altered with $N_{leaf}^{max}$ and $N_{tree}$.(A): Bootstrap re-sampling disabled. (B): Bootstrap re-sampling enabled}{figure.A.2}{}}
\bibdata{el-ht.bib}
\bibcite{ApproximationConcentration}{{1}{2018}{{Belkin}}{{}}}
\bibcite{Belkin_2019}{{2}{2019}{{Belkin et~al.}}{{Belkin, Hsu, Ma \& Mandal}}}
\bibcite{Belkin_Overfitting}{{3}{2018{a}}{{Belkin et~al.}}{{Belkin, Hsu \& Mitra}}}
\bibcite{UnderstandKernel}{{4}{2018{b}}{{Belkin et~al.}}{{Belkin, Ma \& Mandal}}}
\bibcite{Breiman_2001}{{5}{2001}{{Breiman}}{{}}}
\bibcite{Alfredo2016}{{6}{2016}{{Canziani et~al.}}{{Canziani, Paszke \& Culurciello}}}
\bibcite{Cutler_2001}{{7}{2001}{{Cutler \& Zhao}}{{}}}
\bibcite{LossFATBound}{{8}{2001}{{K{\'{e}}gl et~al.}}{{K{\'{e}}gl, Linder \& Lugosi}}}
\bibcite{krizhevsky2009learning}{{9}{2009}{{Krizhevsky \& Hinton}}{{}}}
\bibcite{RFF_Rahimi}{{10}{2008}{{Rahimi \& Recht}}{{}}}
\bibcite{Rudi_2015}{{11}{2015}{{Rudi et~al.}}{{Rudi, Camoriano \& Rosasco}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{43}{section*.5}}
\bibcite{Rudin_1990}{{12}{1990}{{Rudin}}{{}}}
\bibcite{Russakovsky_2015}{{13}{2015}{{Russakovsky et~al.}}{{Russakovsky, Deng, Su, Krause, Satheesh, Ma, Huang, Karpathy, Khosla, Bernstein, Berg \& Fei-Fei}}}
\bibcite{Representer_Theorem}{{14}{2001}{{Sch\IeC {\"o}lkopf et~al.}}{{Sch\IeC {\"o}lkopf, Herbrich \& Smola}}}
\bibcite{Shalev_Shwartz_2010}{{15}{2010}{{Shalev-Shwartz et~al.}}{{Shalev-Shwartz, Singer, Srebro \& Cotter}}}
\bibcite{steinwartSVM}{{16}{2008}{{Steinwart \& Christmann}}{{}}}
\bibcite{ScatteredDataApproximation}{{17}{2004}{{Wendland}}{{}}}
\bibcite{Random_Forests_2017}{{18}{2015}{{Wyner et~al.}}{{Wyner, Olson, Bleich \& Mease}}}
\bibcite{Zhang_Deep_Learning}{{19}{2016}{{Zhang et~al.}}{{Zhang, Bengio, Hardt, Recht \& Vinyals}}}
\bibstyle{sp}
\memsetcounter{lastsheet}{52}
\memsetcounter{lastpage}{44}
